{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/QkTLXl+CKmn45jVEMc/A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmritSDutta/colab_ml/blob/main/model_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following TensorFlow model optimization , https://www.tensorflow.org/model_optimization/guide\n"
      ],
      "metadata": {
        "id": "qmcg2bnn5BAJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "n7bmZhvHwGJu"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_model_optimization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toTydhes5OSb",
        "outputId": "b5ded252-9c0f-4577-ea1c-4a7cf1bc2b9c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_model_optimization in /usr/local/lib/python3.11/dist-packages (0.8.0)\n",
            "Requirement already satisfied: absl-py~=1.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow_model_optimization) (1.4.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow_model_optimization) (0.1.9)\n",
            "Requirement already satisfied: numpy~=1.23 in /usr/local/lib/python3.11/dist-packages (from tensorflow_model_optimization) (1.26.4)\n",
            "Requirement already satisfied: six~=1.14 in /usr/local/lib/python3.11/dist-packages (from tensorflow_model_optimization) (1.17.0)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree~=0.1.1->tensorflow_model_optimization) (25.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from dm-tree~=0.1.1->tensorflow_model_optimization) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_model_optimization as tfmot\n"
      ],
      "metadata": {
        "id": "O8AguXa73ykK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow_model_optimization.python.core.keras.compat import keras"
      ],
      "metadata": {
        "id": "saBpQNnW5X18"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize the input image so that each pixel value is between 0 and 1.\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0"
      ],
      "metadata": {
        "id": "n72K0Dm5wm3h"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model architecture.\n",
        "model = keras.Sequential([\n",
        "  keras.layers.InputLayer(input_shape=(28, 28)),\n",
        "  keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
        "  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
        "  keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "  keras.layers.Flatten(),\n",
        "  keras.layers.Dense(10)\n",
        "])"
      ],
      "metadata": {
        "id": "dVdc9sVwwsB9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the digit classification model\n",
        "model.compile(optimizer='adam',\n",
        "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_images, train_labels, epochs=4, validation_split=0.1)\n",
        "\n",
        "_, baseline_model_accuracy = model.evaluate(\n",
        "    test_images, test_labels, verbose=0)\n",
        "\n",
        "print('Baseline test accuracy:', baseline_model_accuracy)\n",
        "\n",
        "_, keras_file = tempfile.mkstemp('.h5')\n",
        "keras.models.save_model(model, keras_file, include_optimizer=False)\n",
        "print('Saved baseline model to:', keras_file)"
      ],
      "metadata": {
        "id": "fQzdEZyxwySz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66fd0a9f-409b-4084-ca27-9c6678d5295a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "1688/1688 [==============================] - 31s 16ms/step - loss: 0.3259 - accuracy: 0.9060 - val_loss: 0.1521 - val_accuracy: 0.9605\n",
            "Epoch 2/4\n",
            "1688/1688 [==============================] - 18s 11ms/step - loss: 0.1485 - accuracy: 0.9588 - val_loss: 0.1066 - val_accuracy: 0.9718\n",
            "Epoch 3/4\n",
            "1688/1688 [==============================] - 20s 12ms/step - loss: 0.1000 - accuracy: 0.9714 - val_loss: 0.0799 - val_accuracy: 0.9780\n",
            "Epoch 4/4\n",
            "1688/1688 [==============================] - 18s 11ms/step - loss: 0.0776 - accuracy: 0.9778 - val_loss: 0.0661 - val_accuracy: 0.9820\n",
            "Baseline test accuracy: 0.9771000146865845\n",
            "Saved baseline model to: /tmp/tmpm9p7u5wa.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4b66a918dd9b>:14: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  keras.models.save_model(model, keras_file, include_optimizer=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "YZGLVlHUx0po",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98ab72ef-ec10-4d26-d1c3-d10376415a4a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " reshape (Reshape)           (None, 28, 28, 1)         0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 26, 26, 12)        120       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 13, 13, 12)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2028)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                20290     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20410 (79.73 KB)\n",
            "Trainable params: 20410 (79.73 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tf2onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "ByrXYvtxXfX2",
        "outputId": "ef99388e-a15a-4eec-ca57-f46317227510"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tf2onnx\n",
            "  Downloading tf2onnx-1.16.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from tf2onnx) (1.26.4)\n",
            "Collecting onnx>=1.4.1 (from tf2onnx)\n",
            "  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from tf2onnx) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from tf2onnx) (1.17.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.11/dist-packages (from tf2onnx) (25.2.10)\n",
            "Collecting protobuf~=3.20 (from tf2onnx)\n",
            "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->tf2onnx) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->tf2onnx) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->tf2onnx) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->tf2onnx) (2025.1.31)\n",
            "Downloading tf2onnx-1.16.1-py3-none-any.whl (455 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.8/455.8 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, onnx, tf2onnx\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.4\n",
            "    Uninstalling protobuf-5.29.4:\n",
            "      Successfully uninstalled protobuf-5.29.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "tensorflow-metadata 1.17.1 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed onnx-1.17.0 protobuf-3.20.3 tf2onnx-1.16.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "4c83469a912c4c939b756a51531b536e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ONNX quatization"
      ],
      "metadata": {
        "id": "W850d_LdYlPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tf2onnx\n",
        "import onnx\n",
        "import numpy as np\n",
        "\n",
        "spec = (tf.TensorSpec((1, 28, 28, 1), np.float32, name=\"input\"),)\n",
        "model_proto, _ = tf2onnx.convert.from_keras(model, input_signature=spec, opset=13, output_path=\"tf_mnist.onnx\")\n",
        "\n",
        "onnx_model = onnx.load(\"tf_mnist.onnx\")\n",
        "onnx.checker.check_model(onnx_model)"
      ],
      "metadata": {
        "id": "dcgDPOZ1XcSf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4GhnIMPY16w",
        "outputId": "80fa7925-6216-43dc-eee2-e236364ae9dc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.21.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime as ort\n",
        "\n",
        "ort_session = ort.InferenceSession(\"tf_mnist.onnx\")\n",
        "outputs = ort_session.run(None, {\"input\": np.random.randn(1, 28, 28,1).astype(np.float32)})\n",
        "print(\"Output shape:\", outputs[0].shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCua5trAYok2",
        "outputId": "4063f8ab-58bc-4a4d-ce33-8968910b0860"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: (1, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gzipped_model_size(file)-> int:\n",
        "    # Returns size of gzipped model, in bytes.\n",
        "    import os\n",
        "    import zipfile\n",
        "\n",
        "    _, zipped_file = tempfile.mkstemp('.zip')\n",
        "    with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "      f.write(file)\n",
        "\n",
        "    return os.path.getsize(zipped_file)"
      ],
      "metadata": {
        "id": "bj1pMDWRZ6J_"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Size of gzipped pruned model without stripping:{get_gzipped_model_size(keras_file)} bytes')\n",
        "print(f'Size of gzipped pruned model with stripping:{get_gzipped_model_size(\"tf_mnist.onnx\")} bytes')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpnarYiuZ_SG",
        "outputId": "7efc55f4-0c75-4f40-9db9-315dc5a8f361"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of gzipped pruned model without stripping:78177 bytes\n",
            "Size of gzipped pruned model with stripping:76907 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n"
      ],
      "metadata": {
        "id": "bZyGc8xjxLuo"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Compute end step to finish pruning after 2 epochs.\n",
        "batch_size = 128\n",
        "epochs = 2\n",
        "validation_split = 0.1  # 10% of training set will be used for validation set.\n",
        "\n",
        "num_images = train_images.shape[0] * (1 - validation_split)\n",
        "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
        "\n",
        "# Define model for pruning.\n",
        "pruning_params = {\n",
        "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
        "                                                             final_sparsity=0.80,\n",
        "                                                             begin_step=0,\n",
        "                                                             end_step=end_step)\n",
        "}"
      ],
      "metadata": {
        "id": "mTKhP5TzxcqA"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
        "\n",
        "# `prune_low_magnitude` requires a recompile.\n",
        "model_for_pruning.compile(optimizer='adam',\n",
        "                          loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                          metrics=['accuracy'])\n",
        "\n",
        "model_for_pruning.summary()"
      ],
      "metadata": {
        "id": "mcXKEUI-xjXz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "475a9f9a-ec70-49cd-ede0-09e9c7c0080f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " prune_low_magnitude_reshap  (None, 28, 28, 1)         1         \n",
            " e (PruneLowMagnitude)                                           \n",
            "                                                                 \n",
            " prune_low_magnitude_conv2d  (None, 26, 26, 12)        230       \n",
            "  (PruneLowMagnitude)                                            \n",
            "                                                                 \n",
            " prune_low_magnitude_max_po  (None, 13, 13, 12)        1         \n",
            " oling2d (PruneLowMagnitude                                      \n",
            " )                                                               \n",
            "                                                                 \n",
            " prune_low_magnitude_flatte  (None, 2028)              1         \n",
            " n (PruneLowMagnitude)                                           \n",
            "                                                                 \n",
            " prune_low_magnitude_dense   (None, 10)                40572     \n",
            " (PruneLowMagnitude)                                             \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 40805 (159.41 KB)\n",
            "Trainable params: 20410 (79.73 KB)\n",
            "Non-trainable params: 20395 (79.69 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logdir = tempfile.mkdtemp()\n",
        "\n",
        "callbacks = [\n",
        "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
        "]\n",
        "\n",
        "model_for_pruning.fit(train_images, train_labels,\n",
        "                  batch_size=batch_size, epochs=epochs, validation_split=validation_split,\n",
        "                  callbacks=callbacks)"
      ],
      "metadata": {
        "id": "9fqY2knzyTWo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52510966-1666-40f3-cca2-727baee49031"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "  5/422 [..............................] - ETA: 12s - loss: 0.0631 - accuracy: 0.9797"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0295s vs `on_train_batch_end` time: 0.0467s). Check your callbacks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "422/422 [==============================] - 19s 34ms/step - loss: 0.0969 - accuracy: 0.9737 - val_loss: 0.0962 - val_accuracy: 0.9762\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 14s 33ms/step - loss: 0.1146 - accuracy: 0.9678 - val_loss: 0.0953 - val_accuracy: 0.9752\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf_keras.src.callbacks.History at 0x784ee064f410>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, baseline_model_accuracy = model.evaluate(\n",
        "    test_images, test_labels, verbose=0)\n",
        "_, model_for_pruning_accuracy = model_for_pruning.evaluate(\n",
        "   test_images, test_labels, verbose=0)\n",
        "\n",
        "print('Baseline test accuracy:', baseline_model_accuracy)\n",
        "print('Pruned test accuracy:', model_for_pruning_accuracy)"
      ],
      "metadata": {
        "id": "CUiLaCa3xoUy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2a6d6e0-5982-4091-b71b-1177573df269"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline test accuracy: 0.9678999781608582\n",
            "Pruned test accuracy: 0.9678999781608582\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gzipped_model_size(file)-> int:\n",
        "    # Returns size of gzipped model, in bytes.\n",
        "    import os\n",
        "    import zipfile\n",
        "\n",
        "    _, zipped_file = tempfile.mkstemp('.zip')\n",
        "    with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "      f.write(file)\n",
        "\n",
        "    return os.path.getsize(zipped_file)"
      ],
      "metadata": {
        "id": "xuVgykCh2Am5"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "strip_pruning reduces the model size, not prior steps\n"
      ],
      "metadata": {
        "id": "MrkjM9IiJxAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
        "\n",
        "print(\"final model\")\n",
        "model_for_export.summary()\n",
        "_, pruned_keras_file = tempfile.mkstemp('.h5')\n",
        "keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n",
        "print('Saved pruned Keras model to:', pruned_keras_file)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(f'Size of gzipped pruned model without stripping:{get_gzipped_model_size(keras_file)} bytes')\n",
        "print(f'Size of gzipped pruned model with stripping:{get_gzipped_model_size(pruned_keras_file)} bytes')\n"
      ],
      "metadata": {
        "id": "bKPfvhvB1PHo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "580cc013-5f23-4ced-919f-51eb736018b1"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final model\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " reshape (Reshape)           (None, 28, 28, 1)         0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 26, 26, 12)        120       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 13, 13, 12)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2028)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                20290     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20410 (79.73 KB)\n",
            "Trainable params: 20410 (79.73 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-37-aa5cc6dad49d>:6: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved pruned Keras model to: /tmp/tmpyjjncs0a.h5\n",
            "\n",
            "\n",
            "Size of gzipped pruned model without stripping:78177 bytes\n",
            "Size of gzipped pruned model with stripping:25798 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
        "pruned_tflite_model = converter.convert()\n",
        "\n",
        "_, pruned_tflite_file = tempfile.mkstemp('.tflite')\n",
        "\n",
        "with open(pruned_tflite_file, 'wb') as f:\n",
        "  f.write(pruned_tflite_model)\n",
        "\n",
        "print('Saved pruned TFLite model to:', pruned_tflite_file)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(f'Size of gzipped keras model without stripping:{get_gzipped_model_size(keras_file)} bytes')\n",
        "print(f'Size of gzipped pruned model with stripping:{get_gzipped_model_size(pruned_keras_file)} bytes')\n",
        "print(f'Size of gzipped pruned TFlite  model:{get_gzipped_model_size(pruned_tflite_file)} bytes')"
      ],
      "metadata": {
        "id": "b3WZBTqo30Eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc1c6db1-4c0a-4334-85f0-935c8a823328"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved pruned TFLite model to: /tmp/tmpf5iaft3t.tflite\n",
            "\n",
            "\n",
            "Size of gzipped keras model without stripping:78177 bytes\n",
            "Size of gzipped pruned model with stripping:25798 bytes\n",
            "Size of gzipped pruned TFlite  model:24759 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "quantized_and_pruned_tflite_model = converter.convert()\n",
        "\n",
        "_, quantized_and_pruned_tflite_file = tempfile.mkstemp('.tflite')\n",
        "\n",
        "with open(quantized_and_pruned_tflite_file, 'wb') as f:\n",
        "  f.write(quantized_and_pruned_tflite_model)\n",
        "\n",
        "print('Saved quantized and pruned TFLite model to:', quantized_and_pruned_tflite_file)\n",
        "\n",
        "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
        "print(f'Size of gzipped pruned model with stripping:{get_gzipped_model_size(pruned_keras_file)} bytes')\n",
        "print(f'Size of gzipped pruned TFlite  model:{get_gzipped_model_size(pruned_tflite_file)} bytes')\n",
        "print(\"Size of gzipped pruned and quantized TFlite model: %.2f bytes\" % (get_gzipped_model_size(quantized_and_pruned_tflite_file)))"
      ],
      "metadata": {
        "id": "b5FW42a04MBz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cf1df7b-53aa-4702-a342-f2a15abb4b73"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved quantized and pruned TFLite model to: /tmp/tmpj24zbqwl.tflite\n",
            "Size of gzipped baseline Keras model: 78177.00 bytes\n",
            "Size of gzipped pruned model with stripping:25798 bytes\n",
            "Size of gzipped pruned TFlite  model:24759 bytes\n",
            "Size of gzipped pruned and quantized TFlite model: 8473.00 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate_model(interpreter):\n",
        "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "  # Run predictions on ever y image in the \"test\" dataset.\n",
        "  prediction_digits = []\n",
        "  for i, test_image in enumerate(test_images):\n",
        "    if i % 1000 == 0:\n",
        "      print('Evaluated on {n} results so far.'.format(n=i))\n",
        "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "    # the model's input data format.\n",
        "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "    interpreter.set_tensor(input_index, test_image)\n",
        "\n",
        "    # Run inference.\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Post-processing: remove batch dimension and find the digit with highest\n",
        "    # probability.\n",
        "    output = interpreter.tensor(output_index)\n",
        "    digit = np.argmax(output()[0])\n",
        "    prediction_digits.append(digit)\n",
        "\n",
        "  print('\\n')\n",
        "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "  prediction_digits = np.array(prediction_digits)\n",
        "  accuracy = (prediction_digits == test_labels).mean()\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "XOquSrI04h9o"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter = tf.lite.Interpreter(model_content=quantized_and_pruned_tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "test_accuracy = evaluate_model(interpreter)\n",
        "\n",
        "print('Pruned and quantized TFLite test_accuracy:', test_accuracy)\n",
        "print('Pruned TF test accuracy:', model_for_pruning_accuracy)"
      ],
      "metadata": {
        "id": "tZD5W0YJ4jII",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd7c6322-f302-4c3b-e1b6-03459a7e80f0"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluated on 0 results so far.\n",
            "Evaluated on 1000 results so far.\n",
            "Evaluated on 2000 results so far.\n",
            "Evaluated on 3000 results so far.\n",
            "Evaluated on 4000 results so far.\n",
            "Evaluated on 5000 results so far.\n",
            "Evaluated on 6000 results so far.\n",
            "Evaluated on 7000 results so far.\n",
            "Evaluated on 8000 results so far.\n",
            "Evaluated on 9000 results so far.\n",
            "\n",
            "\n",
            "Pruned and quantized TFLite test_accuracy: 0.9681\n",
            "Pruned TF test accuracy: 0.9678999781608582\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.transforms.functional as F"
      ],
      "metadata": {
        "id": "glMcHguKAe37"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "C4jAwXiZBIBH"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_torch = nn.Sequential(\n",
        "    nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # [B, 1, 28, 28] -> [B, 32, 28, 28]\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2, 2),                                   # -> [B, 32, 14, 14]\n",
        "\n",
        "    nn.Conv2d(32, 64, kernel_size=3),                     # -> [B, 64, 12, 12]\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2, 2),                                   # -> [B, 64, 6, 6]\n",
        "\n",
        "    nn.Flatten(),                                         # -> [B, 64*6*6 = 2304]\n",
        "    nn.Linear(64 * 6 * 6, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 10),\n",
        "    nn.LogSoftmax(dim=1)\n",
        ")\n",
        "model_torch = model_torch.to(device)"
      ],
      "metadata": {
        "id": "l0uhiF7ZBPkB"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "from torch.utils.data import DataLoader\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=1000)"
      ],
      "metadata": {
        "id": "OSZvYxC27eFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cd5b96f-8e9a-4d9f-9954-0487d7217200"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 15.9MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 481kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.41MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.28MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.fx\n",
        "\n",
        "\n",
        "traced = torch.fx.symbolic_trace(model_torch)\n",
        "\n",
        "print(traced.graph)  # Prints the ops and tensors flowing through"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyEY3kC99Vg8",
        "outputId": "3a75fb22-2225-4156-953e-e2d7b40d6fa8"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "graph():\n",
            "    %input_1 : [num_users=1] = placeholder[target=input]\n",
            "    %_0 : [num_users=1] = call_module[target=0](args = (%input_1,), kwargs = {})\n",
            "    %_1 : [num_users=1] = call_module[target=1](args = (%_0,), kwargs = {})\n",
            "    %_2 : [num_users=1] = call_module[target=2](args = (%_1,), kwargs = {})\n",
            "    %_3 : [num_users=1] = call_module[target=3](args = (%_2,), kwargs = {})\n",
            "    %_4 : [num_users=1] = call_module[target=4](args = (%_3,), kwargs = {})\n",
            "    %_5 : [num_users=1] = call_module[target=5](args = (%_4,), kwargs = {})\n",
            "    %_6 : [num_users=1] = call_module[target=6](args = (%_5,), kwargs = {})\n",
            "    %_7 : [num_users=1] = call_module[target=7](args = (%_6,), kwargs = {})\n",
            "    %_8 : [num_users=1] = call_module[target=8](args = (%_7,), kwargs = {})\n",
            "    %_9 : [num_users=1] = call_module[target=9](args = (%_8,), kwargs = {})\n",
            "    %_10 : [num_users=1] = call_module[target=10](args = (%_9,), kwargs = {})\n",
            "    return _10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model_torch.parameters(), lr=0.001)\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Training loop\n",
        "def train(model_t, loader):\n",
        "    model_t.train()\n",
        "    epoch_loss = 0\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model_t(images)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "    print(f\"Epoch Loss: {avg_loss:.4f}\")\n",
        "for epochs in range(4):\n",
        "  train(model_torch, train_loader)\n",
        "torch.save(model_torch.state_dict(), \"mnist_torch_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyuWT6aW7aKr",
        "outputId": "0d1073ec-5154-484a-9281-6ad388fdb2e8"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch Loss: 0.0001\n",
            "Epoch Loss: 0.0000\n",
            "Epoch Loss: 0.0001\n",
            "Epoch Loss: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "file_path = \"mnist_torch_model.pth\"\n",
        "file_size = os.path.getsize(file_path) / 1e6  # size in MB\n",
        "print(f\"Model size: {file_size:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0dz9OdfMJ7N",
        "outputId": "b1d07309-0d68-46a5-b1e6-4a45950b18a1"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model size: 1.26 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Evaluation\n",
        "# Recreate model architecture as only weights were saved\n",
        "t_model = nn.Sequential(\n",
        "    nn.Conv2d(1, 32, 3, 1, 1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2, 2),\n",
        "    nn.Conv2d(32, 64, 3),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2, 2),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(64 * 6 * 6, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 10),\n",
        "    nn.LogSoftmax(dim=1)\n",
        ")\n",
        "\n",
        "# Load weights\n",
        "t_model.load_state_dict(torch.load(\"mnist_torch_model.pth\"))\n",
        "t_model = t_model.to(device)\n",
        "t_model.eval()\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        output = t_model(images)\n",
        "        pred = output.argmax(dim=1)\n",
        "        correct += pred.eq(labels).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {correct / len(test_loader.dataset):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsnKJCekJtwE",
        "outputId": "ffff6b6f-cfb7-4c99-9b06-3990e7a9419c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9879\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.utils.prune as prune\n",
        "\n",
        "# Apply pruning to specific layers\n",
        "prune.l1_unstructured(t_model[0], name='weight', amount=0.5)  # Conv2d(1, 32, ...)\n",
        "prune.l1_unstructured(t_model[7], name='weight', amount=0.5)  # Linear(2304, 128)\n",
        "\n",
        "# Make pruning permanent (remove hooks)\n",
        "prune.remove(t_model[0], 'weight')\n",
        "prune.remove(t_model[7], 'weight')\n",
        "\n",
        "\n",
        "torch.save(t_model.state_dict(), \"mnist_torch_model_pruned.pth\")"
      ],
      "metadata": {
        "id": "HTJFP0HSNB71"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**prunning does not decrease filesize automatically. it reduces active weights rather**\n"
      ],
      "metadata": {
        "id": "N4DFkg0CJFM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "org_file_path = \"mnist_torch_model.pth\"\n",
        "org_file_size = os.path.getsize(org_file_path) / 1e6  # size in MB\n",
        "print(f\"Model size: {org_file_size:.2f} MB\")\n",
        "prunned_file_path = 'mnist_torch_model_pruned.pth'\n",
        "prunned_file_size = os.path.getsize(prunned_file_path) / 1e6  # size in MB\n",
        "print(f\"Model size: {prunned_file_size:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ko9PGwq9OPdG",
        "outputId": "7f60d270-8f08-4579-c8a3-083194e64524"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model size: 1.26 MB\n",
            "Model size: 1.26 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    t_model, {nn.Linear}, dtype=torch.qint8\n",
        ")\n"
      ],
      "metadata": {
        "id": "Cki3fXgmO0j2"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(quantized_model.state_dict(), \"mnist_pruned_quantized.pth\")\n",
        "\n",
        "# Check size\n",
        "org_file_path = \"mnist_torch_model.pth\"\n",
        "org_file_size = os.path.getsize(org_file_path) / 1e6  # size in MB\n",
        "print(f\"Model size: {org_file_size:.2f} MB\")\n",
        "prunned_file_path = 'mnist_torch_model_pruned.pth'\n",
        "prunned_file_size = os.path.getsize(prunned_file_path) / 1e6  # size in MB\n",
        "print(f\"Model size: {prunned_file_size:.2f} MB\")\n",
        "size = os.path.getsize(\"mnist_pruned_quantized.pth\") / 1e6\n",
        "print(f\"Pruned + Quantized model size: {size:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HO7t4W6LPDQd",
        "outputId": "de2dad17-c1c1-4d18-abf6-5d0fb3fd5e89"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model size: 1.26 MB\n",
            "Model size: 1.26 MB\n",
            "Pruned + Quantized model size: 0.38 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_clean = nn.Sequential(\n",
        "    nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2, 2),\n",
        "    nn.Conv2d(32, 64, kernel_size=3),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2, 2),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(64 * 6 * 6, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 10),\n",
        "    nn.LogSoftmax(dim=1)\n",
        ")\n",
        "\n",
        "# Copy state_dict from pruned model\n",
        "model_clean.load_state_dict(model_torch.state_dict())\n",
        "model_clean.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCzdU1_WQgL2",
        "outputId": "6330f83f-5e0f-48d2-fcfb-ea7a8799429c"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (1): ReLU()\n",
              "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (4): ReLU()\n",
              "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (6): Flatten(start_dim=1, end_dim=-1)\n",
              "  (7): Linear(in_features=2304, out_features=128, bias=True)\n",
              "  (8): ReLU()\n",
              "  (9): Linear(in_features=128, out_features=10, bias=True)\n",
              "  (10): LogSoftmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model_clean, {nn.Linear}, dtype=torch.qint8\n",
        ")"
      ],
      "metadata": {
        "id": "4x8qbFz7Q2Hv"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(quantized_model.state_dict(), \"mnist_pruned_quantized.pth\")\n",
        "\n",
        "# Check size\n",
        "org_file_path = \"mnist_torch_model.pth\"\n",
        "org_file_size = os.path.getsize(org_file_path) / 1e6  # size in MB\n",
        "print(f\"Model size: {org_file_size:.2f} MB\")\n",
        "prunned_file_path = 'mnist_torch_model_pruned.pth'\n",
        "prunned_file_size = os.path.getsize(prunned_file_path) / 1e6  # size in MB\n",
        "print(f\"Model size: {prunned_file_size:.2f} MB\")\n",
        "size = os.path.getsize(\"mnist_pruned_quantized.pth\") / 1e6\n",
        "print(f\"Pruned + Quantized model size: {size:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJrXErQpRFQG",
        "outputId": "6a2064a8-dc05-4e49-f85f-20ec7b52001a"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model size: 1.26 MB\n",
            "Model size: 1.26 MB\n",
            "Pruned + Quantized model size: 0.38 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model = quantized_model.to('cpu')\n",
        "quantized_model.eval()\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        #images, labels = images.to(device), labels.to(device)\n",
        "        output = quantized_model(images)\n",
        "        pred = output.argmax(dim=1)\n",
        "        correct += pred.eq(labels).sum().item()\n",
        "\n",
        "print(f\"Quantized model accuracy: {correct / len(test_loader.dataset):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fP-R0FHpPOLO",
        "outputId": "3f02a0fd-f1a8-4729-aa64-ea0215b7de04"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantized model accuracy: 0.9876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_input = torch.randn(1, 1, 28, 28)\n",
        "\n",
        "torch.onnx.export(\n",
        "    model_torch,\n",
        "    dummy_input,\n",
        "    \"model_torch_mnist_onnx.onnx\",\n",
        "    input_names=[\"input\"],\n",
        "    output_names=[\"output\"],\n",
        "    opset_version=11\n",
        ")"
      ],
      "metadata": {
        "id": "xXiOHd6necQY"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_model = onnx.load(\"model_torch_mnist_onnx.onnx\")\n",
        "onnx.checker.check_model(onnx_model)\n",
        "\n",
        "ort_session = ort.InferenceSession(\"model_torch_mnist_onnx\")\n",
        "outputs = ort_session.run(None, {\"input\": np.random.randn(1, 1, 28, 28).astype(np.float32)})\n",
        "print(\"Output shape:\", outputs[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "go6SiqVneumX",
        "outputId": "1418dfe4-944d-4c5c-a51e-95914f6c0813"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: (1, 10)\n"
          ]
        }
      ]
    }
  ]
}