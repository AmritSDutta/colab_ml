{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMjX+UfFCWqvVtRHn0D5S9E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmritSDutta/colab_ml/blob/main/model_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following TensorFlow model optimization , https://www.tensorflow.org/model_optimization/guide\n"
      ],
      "metadata": {
        "id": "qmcg2bnn5BAJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7bmZhvHwGJu"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_model_optimization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toTydhes5OSb",
        "outputId": "53bfd67d-e43d-4574-c8d9-c4f186f648a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_model_optimization\n",
            "  Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl.metadata (904 bytes)\n",
            "Requirement already satisfied: absl-py~=1.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow_model_optimization) (1.4.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow_model_optimization) (0.1.9)\n",
            "Collecting numpy~=1.23 (from tensorflow_model_optimization)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six~=1.14 in /usr/local/lib/python3.11/dist-packages (from tensorflow_model_optimization) (1.17.0)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree~=0.1.1->tensorflow_model_optimization) (25.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from dm-tree~=0.1.1->tensorflow_model_optimization) (1.17.2)\n",
            "Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, tensorflow_model_optimization\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4 tensorflow_model_optimization-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_model_optimization as tfmot\n"
      ],
      "metadata": {
        "id": "O8AguXa73ykK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow_model_optimization.python.core.keras.compat import keras"
      ],
      "metadata": {
        "id": "saBpQNnW5X18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize the input image so that each pixel value is between 0 and 1.\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0"
      ],
      "metadata": {
        "id": "n72K0Dm5wm3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model architecture.\n",
        "model = keras.Sequential([\n",
        "  keras.layers.InputLayer(input_shape=(28, 28)),\n",
        "  keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
        "  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
        "  keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "  keras.layers.Flatten(),\n",
        "  keras.layers.Dense(10)\n",
        "])"
      ],
      "metadata": {
        "id": "dVdc9sVwwsB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the digit classification model\n",
        "model.compile(optimizer='adam',\n",
        "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_images, train_labels, epochs=4, validation_split=0.1)\n",
        "\n",
        "_, baseline_model_accuracy = model.evaluate(\n",
        "    test_images, test_labels, verbose=0)\n",
        "\n",
        "print('Baseline test accuracy:', baseline_model_accuracy)\n",
        "\n",
        "_, keras_file = tempfile.mkstemp('.h5')\n",
        "keras.models.save_model(model, keras_file, include_optimizer=False)\n",
        "print('Saved baseline model to:', keras_file)"
      ],
      "metadata": {
        "id": "fQzdEZyxwySz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1981e54-c82d-4eb3-f4aa-e8f9af1c2b24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "1688/1688 [==============================] - 28s 15ms/step - loss: 0.2942 - accuracy: 0.9169 - val_loss: 0.1190 - val_accuracy: 0.9677\n",
            "Epoch 2/4\n",
            "1688/1688 [==============================] - 19s 12ms/step - loss: 0.1250 - accuracy: 0.9636 - val_loss: 0.0921 - val_accuracy: 0.9760\n",
            "Epoch 3/4\n",
            "1688/1688 [==============================] - 19s 11ms/step - loss: 0.0942 - accuracy: 0.9723 - val_loss: 0.0766 - val_accuracy: 0.9783\n",
            "Epoch 4/4\n",
            "1688/1688 [==============================] - 18s 11ms/step - loss: 0.0768 - accuracy: 0.9778 - val_loss: 0.0720 - val_accuracy: 0.9818\n",
            "Baseline test accuracy: 0.9768999814987183\n",
            "Saved baseline model to: /tmp/tmp1erru4iw.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-4b66a918dd9b>:14: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  keras.models.save_model(model, keras_file, include_optimizer=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "YZGLVlHUx0po",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdd9cde0-062f-46a9-e0ab-96a4d3dff713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " reshape (Reshape)           (None, 28, 28, 1)         0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 26, 26, 12)        120       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 13, 13, 12)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2028)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                20290     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20410 (79.73 KB)\n",
            "Trainable params: 20410 (79.73 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n"
      ],
      "metadata": {
        "id": "bZyGc8xjxLuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Compute end step to finish pruning after 2 epochs.\n",
        "batch_size = 128\n",
        "epochs = 2\n",
        "validation_split = 0.1  # 10% of training set will be used for validation set.\n",
        "\n",
        "num_images = train_images.shape[0] * (1 - validation_split)\n",
        "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
        "\n",
        "# Define model for pruning.\n",
        "pruning_params = {\n",
        "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
        "                                                             final_sparsity=0.80,\n",
        "                                                             begin_step=0,\n",
        "                                                             end_step=end_step)\n",
        "}"
      ],
      "metadata": {
        "id": "mTKhP5TzxcqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
        "\n",
        "# `prune_low_magnitude` requires a recompile.\n",
        "model_for_pruning.compile(optimizer='adam',\n",
        "                          loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                          metrics=['accuracy'])\n",
        "\n",
        "model_for_pruning.summary()"
      ],
      "metadata": {
        "id": "mcXKEUI-xjXz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b79f186-07f6-463b-d13b-157b86d216b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " prune_low_magnitude_reshap  (None, 28, 28, 1)         1         \n",
            " e (PruneLowMagnitude)                                           \n",
            "                                                                 \n",
            " prune_low_magnitude_conv2d  (None, 26, 26, 12)        230       \n",
            "  (PruneLowMagnitude)                                            \n",
            "                                                                 \n",
            " prune_low_magnitude_max_po  (None, 13, 13, 12)        1         \n",
            " oling2d (PruneLowMagnitude                                      \n",
            " )                                                               \n",
            "                                                                 \n",
            " prune_low_magnitude_flatte  (None, 2028)              1         \n",
            " n (PruneLowMagnitude)                                           \n",
            "                                                                 \n",
            " prune_low_magnitude_dense   (None, 10)                40572     \n",
            " (PruneLowMagnitude)                                             \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 40805 (159.41 KB)\n",
            "Trainable params: 20410 (79.73 KB)\n",
            "Non-trainable params: 20395 (79.69 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logdir = tempfile.mkdtemp()\n",
        "\n",
        "callbacks = [\n",
        "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
        "]\n",
        "\n",
        "model_for_pruning.fit(train_images, train_labels,\n",
        "                  batch_size=batch_size, epochs=epochs, validation_split=validation_split,\n",
        "                  callbacks=callbacks)"
      ],
      "metadata": {
        "id": "9fqY2knzyTWo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07ccfa95-e897-46a8-cc62-5693e8f4d5eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "422/422 [==============================] - 16s 30ms/step - loss: 0.1154 - accuracy: 0.9667 - val_loss: 0.1497 - val_accuracy: 0.9617\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 13s 30ms/step - loss: 0.1248 - accuracy: 0.9659 - val_loss: 0.0974 - val_accuracy: 0.9733\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf_keras.src.callbacks.History at 0x7aa2f013a350>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, baseline_model_accuracy = model.evaluate(\n",
        "    test_images, test_labels, verbose=0)\n",
        "_, model_for_pruning_accuracy = model_for_pruning.evaluate(\n",
        "   test_images, test_labels, verbose=0)\n",
        "\n",
        "print('Baseline test accuracy:', baseline_model_accuracy)\n",
        "print('Pruned test accuracy:', model_for_pruning_accuracy)"
      ],
      "metadata": {
        "id": "CUiLaCa3xoUy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b4083af-cd3a-477d-ea22-0a3ff602a77e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline test accuracy: 0.9688000082969666\n",
            "Pruned test accuracy: 0.9688000082969666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gzipped_model_size(file)-> int:\n",
        "    # Returns size of gzipped model, in bytes.\n",
        "    import os\n",
        "    import zipfile\n",
        "\n",
        "    _, zipped_file = tempfile.mkstemp('.zip')\n",
        "    with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "      f.write(file)\n",
        "\n",
        "    return os.path.getsize(zipped_file)"
      ],
      "metadata": {
        "id": "xuVgykCh2Am5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "strip_pruning reduces the model size, not prior steps\n"
      ],
      "metadata": {
        "id": "MrkjM9IiJxAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
        "\n",
        "print(\"final model\")\n",
        "model_for_export.summary()\n",
        "_, pruned_keras_file = tempfile.mkstemp('.h5')\n",
        "keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n",
        "print('Saved pruned Keras model to:', pruned_keras_file)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(f'Size of gzipped pruned model without stripping:{get_gzipped_model_size(keras_file)} bytes')\n",
        "print(f'Size of gzipped pruned model with stripping:{get_gzipped_model_size(pruned_keras_file)} bytes')\n"
      ],
      "metadata": {
        "id": "bKPfvhvB1PHo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e7f9b11-35ad-4c2b-8906-b836a80ecaa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final model\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " reshape (Reshape)           (None, 28, 28, 1)         0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 26, 26, 12)        120       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 13, 13, 12)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2028)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                20290     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20410 (79.73 KB)\n",
            "Trainable params: 20410 (79.73 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-aa5cc6dad49d>:6: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved pruned Keras model to: /tmp/tmp05zgvc4f.h5\n",
            "\n",
            "\n",
            "Size of gzipped pruned model without stripping:78194 bytes\n",
            "Size of gzipped pruned model with stripping:25799 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
        "pruned_tflite_model = converter.convert()\n",
        "\n",
        "_, pruned_tflite_file = tempfile.mkstemp('.tflite')\n",
        "\n",
        "with open(pruned_tflite_file, 'wb') as f:\n",
        "  f.write(pruned_tflite_model)\n",
        "\n",
        "print('Saved pruned TFLite model to:', pruned_tflite_file)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(f'Size of gzipped keras model without stripping:{get_gzipped_model_size(keras_file)} bytes')\n",
        "print(f'Size of gzipped pruned model with stripping:{get_gzipped_model_size(pruned_keras_file)} bytes')\n",
        "print(f'Size of gzipped pruned TFlite  model:{get_gzipped_model_size(pruned_tflite_file)} bytes')"
      ],
      "metadata": {
        "id": "b3WZBTqo30Eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88943143-7dad-4214-c684-b9864bb04b11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved pruned TFLite model to: /tmp/tmp6t08lofn.tflite\n",
            "\n",
            "\n",
            "Size of gzipped keras model without stripping:78194 bytes\n",
            "Size of gzipped pruned model with stripping:25799 bytes\n",
            "Size of gzipped pruned TFlite  model:24717 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "quantized_and_pruned_tflite_model = converter.convert()\n",
        "\n",
        "_, quantized_and_pruned_tflite_file = tempfile.mkstemp('.tflite')\n",
        "\n",
        "with open(quantized_and_pruned_tflite_file, 'wb') as f:\n",
        "  f.write(quantized_and_pruned_tflite_model)\n",
        "\n",
        "print('Saved quantized and pruned TFLite model to:', quantized_and_pruned_tflite_file)\n",
        "\n",
        "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
        "print(f'Size of gzipped pruned model with stripping:{get_gzipped_model_size(pruned_keras_file)} bytes')\n",
        "print(f'Size of gzipped pruned TFlite  model:{get_gzipped_model_size(pruned_tflite_file)} bytes')\n",
        "print(\"Size of gzipped pruned and quantized TFlite model: %.2f bytes\" % (get_gzipped_model_size(quantized_and_pruned_tflite_file)))"
      ],
      "metadata": {
        "id": "b5FW42a04MBz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45447f6f-59c7-44db-f5a1-b52d66443baa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved quantized and pruned TFLite model to: /tmp/tmp9z75fzm_.tflite\n",
            "Size of gzipped baseline Keras model: 78194.00 bytes\n",
            "Size of gzipped pruned model with stripping:25799 bytes\n",
            "Size of gzipped pruned TFlite  model:24717 bytes\n",
            "Size of gzipped pruned and quantized TFlite model: 8433.00 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate_model(interpreter):\n",
        "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "  # Run predictions on ever y image in the \"test\" dataset.\n",
        "  prediction_digits = []\n",
        "  for i, test_image in enumerate(test_images):\n",
        "    if i % 1000 == 0:\n",
        "      print('Evaluated on {n} results so far.'.format(n=i))\n",
        "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "    # the model's input data format.\n",
        "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "    interpreter.set_tensor(input_index, test_image)\n",
        "\n",
        "    # Run inference.\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Post-processing: remove batch dimension and find the digit with highest\n",
        "    # probability.\n",
        "    output = interpreter.tensor(output_index)\n",
        "    digit = np.argmax(output()[0])\n",
        "    prediction_digits.append(digit)\n",
        "\n",
        "  print('\\n')\n",
        "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "  prediction_digits = np.array(prediction_digits)\n",
        "  accuracy = (prediction_digits == test_labels).mean()\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "XOquSrI04h9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter = tf.lite.Interpreter(model_content=quantized_and_pruned_tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "test_accuracy = evaluate_model(interpreter)\n",
        "\n",
        "print('Pruned and quantized TFLite test_accuracy:', test_accuracy)\n",
        "print('Pruned TF test accuracy:', model_for_pruning_accuracy)"
      ],
      "metadata": {
        "id": "tZD5W0YJ4jII",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b140831-0fca-4045-9c95-e247febf75b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluated on 0 results so far.\n",
            "Evaluated on 1000 results so far.\n",
            "Evaluated on 2000 results so far.\n",
            "Evaluated on 3000 results so far.\n",
            "Evaluated on 4000 results so far.\n",
            "Evaluated on 5000 results so far.\n",
            "Evaluated on 6000 results so far.\n",
            "Evaluated on 7000 results so far.\n",
            "Evaluated on 8000 results so far.\n",
            "Evaluated on 9000 results so far.\n",
            "\n",
            "\n",
            "Pruned and quantized TFLite test_accuracy: 0.9688\n",
            "Pruned TF test accuracy: 0.9688000082969666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.transforms.functional as F"
      ],
      "metadata": {
        "id": "glMcHguKAe37"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "C4jAwXiZBIBH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_torch = nn.Sequential(\n",
        "    nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # [B, 1, 28, 28] -> [B, 32, 28, 28]\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2, 2),                                   # -> [B, 32, 14, 14]\n",
        "\n",
        "    nn.Conv2d(32, 64, kernel_size=3),                     # -> [B, 64, 12, 12]\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2, 2),                                   # -> [B, 64, 6, 6]\n",
        "\n",
        "    nn.Flatten(),                                         # -> [B, 64*6*6 = 2304]\n",
        "    nn.Linear(64 * 6 * 6, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 10),\n",
        "    nn.LogSoftmax(dim=1)\n",
        ")\n",
        "model_torch = model_torch.to(device)"
      ],
      "metadata": {
        "id": "l0uhiF7ZBPkB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "from torch.utils.data import DataLoader\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=1000)"
      ],
      "metadata": {
        "id": "OSZvYxC27eFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10a240fe-7874-4953-d414-461dc3d94f6f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:11<00:00, 899kB/s] \n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 64.8kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.25MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.78MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.fx\n",
        "\n",
        "\n",
        "traced = torch.fx.symbolic_trace(model_torch)\n",
        "\n",
        "print(traced.graph)  # Prints the ops and tensors flowing through"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyEY3kC99Vg8",
        "outputId": "198974da-e758-46d3-f671-80b229b73b32"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "graph():\n",
            "    %input_1 : [num_users=1] = placeholder[target=input]\n",
            "    %_0 : [num_users=1] = call_module[target=0](args = (%input_1,), kwargs = {})\n",
            "    %_1 : [num_users=1] = call_module[target=1](args = (%_0,), kwargs = {})\n",
            "    %_2 : [num_users=1] = call_module[target=2](args = (%_1,), kwargs = {})\n",
            "    %_3 : [num_users=1] = call_module[target=3](args = (%_2,), kwargs = {})\n",
            "    %_4 : [num_users=1] = call_module[target=4](args = (%_3,), kwargs = {})\n",
            "    %_5 : [num_users=1] = call_module[target=5](args = (%_4,), kwargs = {})\n",
            "    %_6 : [num_users=1] = call_module[target=6](args = (%_5,), kwargs = {})\n",
            "    %_7 : [num_users=1] = call_module[target=7](args = (%_6,), kwargs = {})\n",
            "    %_8 : [num_users=1] = call_module[target=8](args = (%_7,), kwargs = {})\n",
            "    %_9 : [num_users=1] = call_module[target=9](args = (%_8,), kwargs = {})\n",
            "    %_10 : [num_users=1] = call_module[target=10](args = (%_9,), kwargs = {})\n",
            "    return _10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model_torch.parameters(), lr=0.001)\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Training loop\n",
        "def train(model_t, loader):\n",
        "    model_t.train()\n",
        "    epoch_loss = 0\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model_t(images)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "    print(f\"Epoch Loss: {avg_loss:.4f}\")\n",
        "for epochs in range(4):\n",
        "  train(model_torch, train_loader)\n",
        "torch.save(model_torch.state_dict(), \"mnist_torch_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyuWT6aW7aKr",
        "outputId": "c0be19ce-6e4e-4329-d6bc-b48587b1ea79"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch Loss: 0.0000\n",
            "Epoch Loss: 0.0001\n",
            "Epoch Loss: 0.0000\n",
            "Epoch Loss: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "file_path = \"mnist_torch_model.pth\"\n",
        "file_size = os.path.getsize(file_path) / 1e6  # size in MB\n",
        "print(f\"Model size: {file_size:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0dz9OdfMJ7N",
        "outputId": "5abac57f-3237-4a67-bb9b-932d3d1cf620"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model size: 1.26 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Evaluation\n",
        "# Recreate model architecture as only weights were saved\n",
        "t_model = nn.Sequential(\n",
        "    nn.Conv2d(1, 32, 3, 1, 1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2, 2),\n",
        "    nn.Conv2d(32, 64, 3),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2, 2),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(64 * 6 * 6, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 10),\n",
        "    nn.LogSoftmax(dim=1)\n",
        ")\n",
        "\n",
        "# Load weights\n",
        "t_model.load_state_dict(torch.load(\"mnist_torch_model.pth\"))\n",
        "t_model = t_model.to(device)\n",
        "t_model.eval()\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        output = t_model(images)\n",
        "        pred = output.argmax(dim=1)\n",
        "        correct += pred.eq(labels).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {correct / len(test_loader.dataset):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsnKJCekJtwE",
        "outputId": "c053afad-3037-4fc2-ba6d-828eaf9add0c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.utils.prune as prune\n",
        "\n",
        "# Apply pruning to specific layers\n",
        "prune.l1_unstructured(t_model[0], name='weight', amount=0.5)  # Conv2d(1, 32, ...)\n",
        "prune.l1_unstructured(t_model[7], name='weight', amount=0.5)  # Linear(2304, 128)\n",
        "\n",
        "# Make pruning permanent (remove hooks)\n",
        "prune.remove(t_model[0], 'weight')\n",
        "prune.remove(t_model[7], 'weight')\n",
        "\n",
        "\n",
        "torch.save(t_model.state_dict(), \"mnist_torch_model_pruned.pth\")"
      ],
      "metadata": {
        "id": "HTJFP0HSNB71"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**prunning does not decrease filesize automatically. it reduces active weights rather**\n"
      ],
      "metadata": {
        "id": "N4DFkg0CJFM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "org_file_path = \"mnist_torch_model.pth\"\n",
        "org_file_size = os.path.getsize(org_file_path) / 1e6  # size in MB\n",
        "print(f\"Model size: {org_file_size:.2f} MB\")\n",
        "prunned_file_path = 'mnist_torch_model_pruned.pth'\n",
        "prunned_file_size = os.path.getsize(prunned_file_path) / 1e6  # size in MB\n",
        "print(f\"Model size: {prunned_file_size:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ko9PGwq9OPdG",
        "outputId": "7ce4b7f5-0d69-44b0-bd1c-b84b199c543e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model size: 1.26 MB\n",
            "Model size: 1.26 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    t_model, {nn.Linear}, dtype=torch.qint8\n",
        ")\n"
      ],
      "metadata": {
        "id": "Cki3fXgmO0j2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(quantized_model.state_dict(), \"mnist_pruned_quantized.pth\")\n",
        "\n",
        "# Check size\n",
        "org_file_path = \"mnist_torch_model.pth\"\n",
        "org_file_size = os.path.getsize(org_file_path) / 1e6  # size in MB\n",
        "print(f\"Model size: {org_file_size:.2f} MB\")\n",
        "prunned_file_path = 'mnist_torch_model_pruned.pth'\n",
        "prunned_file_size = os.path.getsize(prunned_file_path) / 1e6  # size in MB\n",
        "print(f\"Model size: {prunned_file_size:.2f} MB\")\n",
        "size = os.path.getsize(\"mnist_pruned_quantized.pth\") / 1e6\n",
        "print(f\"Pruned + Quantized model size: {size:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HO7t4W6LPDQd",
        "outputId": "be642f75-0292-47ff-9dd3-26911860ead4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model size: 1.26 MB\n",
            "Model size: 1.26 MB\n",
            "Pruned + Quantized model size: 0.38 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_clean = nn.Sequential(\n",
        "    nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2, 2),\n",
        "    nn.Conv2d(32, 64, kernel_size=3),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2, 2),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(64 * 6 * 6, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 10),\n",
        "    nn.LogSoftmax(dim=1)\n",
        ")\n",
        "\n",
        "# Copy state_dict from pruned model\n",
        "model_clean.load_state_dict(model_torch.state_dict())\n",
        "model_clean.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCzdU1_WQgL2",
        "outputId": "79001269-cdfa-4f6b-af7c-c27e1fb82081"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (1): ReLU()\n",
              "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (4): ReLU()\n",
              "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (6): Flatten(start_dim=1, end_dim=-1)\n",
              "  (7): Linear(in_features=2304, out_features=128, bias=True)\n",
              "  (8): ReLU()\n",
              "  (9): Linear(in_features=128, out_features=10, bias=True)\n",
              "  (10): LogSoftmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model_clean, {nn.Linear}, dtype=torch.qint8\n",
        ")"
      ],
      "metadata": {
        "id": "4x8qbFz7Q2Hv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(quantized_model.state_dict(), \"mnist_pruned_quantized.pth\")\n",
        "\n",
        "# Check size\n",
        "org_file_path = \"mnist_torch_model.pth\"\n",
        "org_file_size = os.path.getsize(org_file_path) / 1e6  # size in MB\n",
        "print(f\"Model size: {org_file_size:.2f} MB\")\n",
        "prunned_file_path = 'mnist_torch_model_pruned.pth'\n",
        "prunned_file_size = os.path.getsize(prunned_file_path) / 1e6  # size in MB\n",
        "print(f\"Model size: {prunned_file_size:.2f} MB\")\n",
        "size = os.path.getsize(\"mnist_pruned_quantized.pth\") / 1e6\n",
        "print(f\"Pruned + Quantized model size: {size:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJrXErQpRFQG",
        "outputId": "ae4947f5-8248-4735-b805-d687e0dcae77"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model size: 1.26 MB\n",
            "Model size: 1.26 MB\n",
            "Pruned + Quantized model size: 0.38 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model = quantized_model.to('cpu')\n",
        "quantized_model.eval()\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        #images, labels = images.to(device), labels.to(device)\n",
        "        output = quantized_model(images)\n",
        "        pred = output.argmax(dim=1)\n",
        "        correct += pred.eq(labels).sum().item()\n",
        "\n",
        "print(f\"Quantized model accuracy: {correct / len(test_loader.dataset):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fP-R0FHpPOLO",
        "outputId": "db0b7466-9daa-4115-b425-fba81ffec022"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantized model accuracy: 0.9884\n"
          ]
        }
      ]
    }
  ]
}